{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-SB89FuKnAx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLl4w_BjLhSS"
   },
   "outputs": [],
   "source": [
    "# !wget --directory-prefix=downloads http://calvin.inf.ed.ac.uk/wp-content/uploads/data/cocostuffdataset/cocostuff-10k-v1.1.zip\n",
    "# !unzip -qq downloads/cocostuff-10k-v1.1.zip -d dataset/\n",
    "# !wget --directory-prefix=dataset/annotations-json http://calvin.inf.ed.ac.uk/wp-content/uploads/data/cocostuffdataset/cocostuff-10k-v1.1.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WNsd2wQEjrbO",
    "outputId": "268f1ff3-fe37-4112-9f2e-c87b95f4457e"
   },
   "outputs": [],
   "source": [
    "pip install torch torchvision pycocotools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BsZs7vjwmvL0",
    "outputId": "bd8da106-19aa-4fb2-e74e-b759a4197773"
   },
   "outputs": [],
   "source": [
    "pip install opencv-python numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Li_ug9eppVcw"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Load COCO-Stuff label mapping from a JSON file\n",
    "def load_coco_stuff_labels(annotation_file):\n",
    "    \"\"\"\n",
    "    Loads the COCO-Stuff label names from an annotation file (usually in JSON format).\n",
    "\n",
    "    Args:\n",
    "    - annotation_file: Path to the COCO-Stuff annotations JSON file.\n",
    "\n",
    "    Returns:\n",
    "    - label_map: Dictionary mapping label IDs to class names.\n",
    "    \"\"\"\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract categories (id and name) from the annotation file\n",
    "    label_map = {category['id']: category['name'] for category in data['categories']}\n",
    "    return label_map\n",
    "\n",
    "# Example: Path to your COCO-Stuff annotations JSON\n",
    "annotation_file = '/content/dataset/annotations-json/cocostuff-10k-v1.1.json'\n",
    "\n",
    "# Load the label map\n",
    "LABELS = load_coco_stuff_labels(annotation_file)\n",
    "\n",
    "def visualize_prediction(image, targets, prediction, idx=0):\n",
    "    \"\"\"\n",
    "    Visualizes the predicted bounding boxes, labels, and scores on an image.\n",
    "\n",
    "    Args:\n",
    "    - image: Tensor representing the image.\n",
    "    - targets: Ground truth (not used here but can be visualized).\n",
    "    - prediction: Model prediction dictionary containing boxes, labels, and scores.\n",
    "    - idx: Index of the image to visualize from the batch.\n",
    "    \"\"\"\n",
    "    # Convert image back from tensor to numpy array (RGB)\n",
    "    img = image[idx].permute(1, 2, 0).cpu().numpy()\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "\n",
    "    # Create a figure and axis for plotting\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # Draw the predicted boxes\n",
    "    boxes = prediction['boxes'].cpu().numpy()\n",
    "    labels = prediction['labels'].cpu().numpy()\n",
    "    scores = prediction['scores'].cpu().numpy()\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if scores[i] > 0.1:  # Display boxes with score > 0.1\n",
    "            rect = patches.Rectangle(\n",
    "                (boxes[i][0], boxes[i][1]), boxes[i][2] - boxes[i][0], boxes[i][3] - boxes[i][1],\n",
    "                linewidth=2, edgecolor='red', facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Get the label name from the mapping\n",
    "            label_name = LABELS.get(labels[i], 'Unknown')  # Default to 'Unknown' if label not in mapping\n",
    "            score = scores[i]\n",
    "            ax.text(\n",
    "                boxes[i][0], boxes[i][1], f'{label_name}: {score:.2f}', color='white',\n",
    "                fontsize=12, bbox=dict(facecolor='red', alpha=0.7)\n",
    "            )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage (assuming you have a batch of images and predictions):\n",
    "# image = ... (a batch of images, tensor format)\n",
    "# targets = ... (ground truth annotations)\n",
    "# prediction = ... (model predictions)\n",
    "\n",
    "# visualize_prediction(image, targets, prediction, idx=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8FBWuCI4kh4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LOhlb8Mk4xyv",
    "outputId": "4eab912a-1f5b-441a-f39b-24d3a7e0c93c"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to the annotations file\n",
    "annotation_file = '/content/dataset/annotations-json/cocostuff-10k-v1.1.json'\n",
    "\n",
    "# Load the JSON file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Print the keys in the JSON file to understand the structure\n",
    "print(\"Top-level keys in the JSON file:\", annotations.keys())\n",
    "\n",
    "# Inspect some image metadata\n",
    "print(\"\\nExample image entry:\")\n",
    "print(json.dumps(annotations['images'][0], indent=4))\n",
    "\n",
    "# Inspect some annotation data\n",
    "print(\"\\nExample annotation entry:\")\n",
    "print(json.dumps(annotations['annotations'][0], indent=4))\n",
    "\n",
    "# Inspect categories (labels)\n",
    "print(\"\\nCategories (label mapping):\")\n",
    "print(json.dumps(annotations['categories'], indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xg4vHED1oB9O",
    "outputId": "45eca595-73c7-4a00-e543-e601ab1329c0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "class CustomDatasetWithJSONAnnotations(Dataset):\n",
    "    def __init__(self, image_dir, annotation_file, image_list_file, transforms=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Load annotations JSON file\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            self.annotations_data = json.load(f)\n",
    "\n",
    "                # Update the initialization of the `image_list`\n",
    "        with open(image_list_file, 'r') as f:\n",
    "            self.image_list = set(line.strip() + '.jpg' for line in f)  # Append '.jpg' to each filename\n",
    "\n",
    "        # Create a list of images based on the provided image list\n",
    "        self.images = [\n",
    "            img for img in self.annotations_data['images']\n",
    "            if img['file_name'] in self.image_list\n",
    "        ]\n",
    "\n",
    "\n",
    "        # Create annotation lookup dictionary\n",
    "        self.annotations = self.annotations_data['annotations']\n",
    "        self.image_to_annotations = {img['id']: [] for img in self.images}\n",
    "        for anno in self.annotations:\n",
    "            if anno['image_id'] in self.image_to_annotations:\n",
    "                self.image_to_annotations[anno['image_id']].append(anno)\n",
    "\n",
    "        # Map categories for label mapping\n",
    "        self.categories = {cat['id']: cat['name'] for cat in self.annotations_data['categories']}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image metadata\n",
    "        image_meta = self.images[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_meta['file_name'])\n",
    "\n",
    "        # Load and preprocess the image\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = torch.tensor(img / 255.0, dtype=torch.float32).permute(2, 0, 1)\n",
    "\n",
    "        # Get annotations for the current image\n",
    "        annotations = self.image_to_annotations[image_meta['id']]\n",
    "\n",
    "        boxes, labels, areas, iscrowd = [], [], [], []\n",
    "        for anno in annotations:\n",
    "            if isinstance(anno['bbox'], list) and len(anno['bbox']) == 4:\n",
    "                x, y, w, h = anno['bbox']\n",
    "                boxes.append([x, y, x + w, y + h])\n",
    "                labels.append(anno['category_id'])\n",
    "                areas.append(anno['area'])\n",
    "                iscrowd.append(anno['iscrowd'])\n",
    "\n",
    "            else:\n",
    "                if isinstance(anno['bbox'], list) and len(anno['bbox']) == 1 and len(anno['bbox'][0]) == 4:\n",
    "                  x, y, w, h = anno['bbox'][0]  # Extract the inner list\n",
    "                  boxes.append([x, y, x + w, y + h])\n",
    "                  labels.append(anno['category_id'])\n",
    "                  areas.append(anno['area'])\n",
    "                  iscrowd.append(anno['iscrowd'])\n",
    "                else:\n",
    "                # Handle cases where 'bbox' is not in the expected format\n",
    "                  print(f\"Warning: Skipping annotation with invalid 'bbox' format: {anno['bbox']}\")\n",
    "                # You can choose to ignore these annotations or handle them differently\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Handle the case where there are no annotations for an image\n",
    "        if not boxes:\n",
    "            return img,None\n",
    "        else:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "            iscrowd = torch.as_tensor(iscrowd, dtype=torch.uint8)\n",
    "\n",
    "        # Create the target dictionary\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"area\": areas,\n",
    "            \"iscrowd\": iscrowd\n",
    "        }\n",
    "\n",
    "        # Apply any transforms if provided\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "\n",
    "# Helper function to visualize an image with bounding boxes\n",
    "def visualize_image_with_boxes(img, target, category_mapping):\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "    img = img.permute(1, 2, 0).cpu().numpy()  # Convert from (C, H, W) to (H, W, C)\n",
    "    ax.imshow(img)\n",
    "\n",
    "    for box, label in zip(target['boxes'], target['labels']):\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "        width = x_max - x_min\n",
    "        height = y_max - y_min\n",
    "        rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='red', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x_min, y_min - 5, category_mapping[label.item()], color='yellow', fontsize=12, weight='bold')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# File paths\n",
    "train_image_dir = \"/content/dataset/images\"\n",
    "val_image_dir = \"/content/dataset/images\"\n",
    "annotation_file = \"/content/dataset/annotations-json/cocostuff-10k-v1.1.json\"\n",
    "train_list_file = \"/content/dataset/imageLists/train.txt\"\n",
    "test_list_file = \"/content/dataset/imageLists/test.txt\"\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = CustomDatasetWithJSONAnnotations(\n",
    "    train_image_dir, annotation_file, train_list_file\n",
    ")\n",
    "test_dataset = CustomDatasetWithJSONAnnotations(\n",
    "    val_image_dir, annotation_file, test_list_file\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# Visualize a batch\n",
    "images, targets = next(iter(train_loader))\n",
    "for i in range(len(images)):\n",
    "    print(f\"Visualizing image {i+1} in the batch:\")\n",
    "    visualize_image_with_boxes(images[i], targets[i], train_dataset.categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Jj0PWvEBaA2",
    "outputId": "5381c95c-b8c3-437a-f717-09b782f0c1e1"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2,FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.resnet import ResNet50_Weights\n",
    "\n",
    "\n",
    "# Load the pretrained Faster R-CNN model and modify it for your dataset\n",
    "  # Number of classes in your dataset (adjust this based on your dataset)\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights_backbone=ResNet50_Weights.DEFAULT,num_classes=183,trainable_backbone_layers=2)\n",
    "model.load_state_dict(torch.load('/content/drive/MyDrive/Faster_RCNN_train_state/faster_rcnn_best_model.pth'))\n",
    "\n",
    "# Modify the model for the new number of classes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clg5IsZuMLny"
   },
   "outputs": [],
   "source": [
    "# Function to evaluate the model on the validation set with visualization\n",
    "def evaluate_model(model, loader, device, visualize=False, num_images_to_visualize=5):\n",
    "\n",
    "\n",
    "       model.eval()\n",
    "       total_loss = 0.0\n",
    "       image_idx = 0\n",
    "       with torch.no_grad():\n",
    "           for images, targets in loader:\n",
    "\n",
    "                # Filter out images with None targets\n",
    "               valid_indices = [i for i, target in enumerate(targets) if target is not None]\n",
    "\n",
    "               if not valid_indices:  # Skip if all targets are None\n",
    "                 continue\n",
    "\n",
    "               images = [images[i] for i in valid_indices]\n",
    "               targets = [targets[i] for i in valid_indices]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "               images = [img.to(device) for img in images]\n",
    "               targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "               model.train()\n",
    "               loss_dict = model(images, targets)\n",
    "               losses = sum(loss for loss in loss_dict.values())\n",
    "               total_loss += losses.item()\n",
    "\n",
    "\n",
    "               model.eval()\n",
    "\n",
    "               if visualize and image_idx < num_images_to_visualize:\n",
    "                # Get model predictions\n",
    "                predictions = model(images)\n",
    "                print(predictions[0])\n",
    "                # Visualize the first image in the batch\n",
    "                visualize_prediction(images, targets, predictions[0], idx=0)\n",
    "                image_idx += 1\n",
    "\n",
    "       avg_loss = total_loss / len(loader)\n",
    "       print(f\"Validation Loss: {avg_loss:.4f}\")\n",
    "       return avg_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQvSPi-RglTU"
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "params = [\n",
    "    # Pre-trained backbone with a low learning rate\n",
    "    {\"params\": model.backbone.parameters(), \"lr\": 1e-5},\n",
    "    # Region Proposal Network (RPN) with a medium learning rate\n",
    "    {\"params\": model.rpn.parameters(), \"lr\": 1e-4},\n",
    "    # RoI heads with a higher learning rate\n",
    "    {\"params\": model.roi_heads.parameters(), \"lr\": 1e-3}\n",
    "]\n",
    "\n",
    "# Define the optimizer with different learning rates for each part\n",
    "optimizer = Adam(params, weight_decay=0.0005)  # Optional: Add L2 regularization\n",
    "\n",
    "# Set up the ReduceLROnPlateau scheduler\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GVjh0G6oGJi2",
    "outputId": "cac6168b-176f-4793-d8f6-c9423f612a00"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch.cuda.amp as amp\n",
    "scaler = amp.GradScaler()\n",
    "# Training loop with validation after each epoch\n",
    "# Training loop with validation and visualization after each epoch\n",
    "num_epochs = 20  # Set the number of epochs based on your available time\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # Training step\n",
    "    for images, targets in train_loader:\n",
    "\n",
    "        # Filter out images with None targets\n",
    "        valid_indices = [i for i, target in enumerate(targets) if target is not None]\n",
    "\n",
    "        if not valid_indices:  # Skip if all targets are None\n",
    "          continue\n",
    "\n",
    "        images = [images[i] for i in valid_indices]\n",
    "        targets = [targets[i] for i in valid_indices]\n",
    "\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        with amp.autocast():\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Scales the loss, performs backward pass, and updates gradients\n",
    "        scaler.scale(losses).backward()\n",
    "\n",
    "        # Update the weights using the scaled gradients\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += losses.item()\n",
    "\n",
    "\n",
    "\n",
    "    # Validate the model after every epoch and visualize predictions\n",
    "    val_loss=evaluate_model(model, test_loader, device, visualize=True)\n",
    "    lr_scheduler.step(val_loss)\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "    # Save the model if it has improved validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'faster_rcnn_best_model.pth')\n",
    "        print(f\"Model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Save the final model after training\n",
    "torch.save(model.state_dict(), 'faster_rcnn_final_model_20_epoch.pth')\n",
    "print(\"Training completed and final model saved.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tnw3G2fkI7D"
   },
   "outputs": [],
   "source": [
    "torch.save(lr_scheduler.state_dict(),'lr_scheduler.pth')\n",
    "torch.save(optimizer.state_dict(),'optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCMCiRCsjEMQ"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SsBlbYmpjExV"
   },
   "outputs": [],
   "source": [
    "!cp '/content/faster_rcnn_best_model.pth' '/content/drive/MyDrive/RCNN_30_Epoch'\n",
    "!cp '/content/lr_scheduler.pth' '/content/drive/MyDrive/RCNN_30_Epoch'\n",
    "!cp '/content/optimizer.pth' '/content/drive/MyDrive/RCNN_30_Epoch'\n",
    "!cp '/content/faster_rcnn_final_model_20_epoch.pth' '/content/drive/MyDrive/RCNN_30_Epoch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 670
    },
    "id": "A1Ze00p0mKlC",
    "outputId": "ca02b596-6f7a-47a9-a41c-a4ea314cefd5"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the class mappings from a JSON file\n",
    "def load_label_mapping(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract the categories and map the id to the name\n",
    "    label_map = {category['id']: category['name'] for category in data['categories']}\n",
    "\n",
    "    return label_map\n",
    "\n",
    "# Function to visualize predictions with labels\n",
    "def visualize_prediction(image, predictions, label_map, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Visualizes the predicted bounding boxes, labels, and scores on an image.\n",
    "    Args:\n",
    "        image (PIL Image or numpy.ndarray): Input image\n",
    "        predictions (dict): Model's predictions (boxes, labels, scores)\n",
    "        label_map (dict): Mapping of label IDs to human-readable class names\n",
    "        threshold (float): Confidence threshold to display predictions\n",
    "    \"\"\"\n",
    "    # Convert image to numpy array if it's a PIL Image\n",
    "    if isinstance(image, Image.Image):\n",
    "        image = np.array(image)\n",
    "\n",
    "    # Create a figure and axis for plotting\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Get predictions\n",
    "    boxes = predictions['boxes'].cpu().numpy()\n",
    "    labels = predictions['labels'].cpu().numpy()\n",
    "    scores = predictions['scores'].cpu().numpy()\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if scores[i] > threshold:  # Filter predictions based on score\n",
    "            rect = patches.Rectangle(\n",
    "                (boxes[i][0], boxes[i][1]),\n",
    "                boxes[i][2] - boxes[i][0],\n",
    "                boxes[i][3] - boxes[i][1],\n",
    "                linewidth=2, edgecolor='red', facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Get the label name from label_map\n",
    "            label_name = label_map.get(labels[i], 'Unknown')\n",
    "\n",
    "            # Add label and score text\n",
    "            ax.text(\n",
    "                boxes[i][0], boxes[i][1] - 5,\n",
    "                f'{label_name}: {scores[i]:.2f}',\n",
    "                color='white', fontsize=12,\n",
    "                bbox=dict(facecolor='red', alpha=0.7)\n",
    "            )\n",
    "\n",
    "    plt.show()\n",
    "    print(label_name)\n",
    "\n",
    "# Load a pretrained Faster R-CNN model (or your custom model)\n",
    "\n",
    "model.load_state_dict(torch.load('/content/faster_rcnn_best_model.pth'))\n",
    "model.eval() # Set the model to evaluation mode\n",
    "# Load the image from file\n",
    "image_path = '/content/U76fMj.webp'  # Change this to the path of your image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Define the necessary transformations for input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "])\n",
    "\n",
    "# Preprocess the image\n",
    "image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Perform inference (predict)\n",
    "with torch.no_grad():\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    predictions = model(image_tensor)\n",
    "\n",
    "# Load label mapping from a JSON file\n",
    "label_map = load_label_mapping('/content/dataset/annotations-json/cocostuff-10k-v1.1.json')  # Update with your JSON path\n",
    "\n",
    "# Visualize the predictions\n",
    "visualize_prediction(image, predictions[0], label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9w_X9jxoTeq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
